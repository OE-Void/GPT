{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OE-Void/GPT/blob/main/model_from_scr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHWTbbCSxBGr"
      },
      "source": [
        "# Lets train model from scratch now\n",
        "\n",
        "- Author - Parvesh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4l7IxZBxPTX"
      },
      "source": [
        "## Define a standard tranformers from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBX9Ta701p5I"
      },
      "source": [
        "- imports needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWqlsr_21pXg"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeH5wYW4jcRe"
      },
      "source": [
        "- hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXDyylDEjcRf"
      },
      "outputs": [],
      "source": [
        "# standard hyperparameters to train and make a standard transformers model\n",
        "\n",
        "batch_size = 32 # number of sequences processed at once\n",
        "block_size = 4096 # maximum context length we took 4096 as dataset have average length of around 800 to 900 tokens if its low like 256 model will not learn about <eos> token\n",
        "max_iters = 800 # maximum number of training steps\n",
        "eval_interval = 200 # evaluate every 200 steps\n",
        "learning_rate = 3e-4 # learning rate\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200 # logging step\n",
        "n_embd = 384 # embedding dimension\n",
        "n_head = 6 # number of attention heads\n",
        "# yoo if you think i haven't defined the head dim its 32 as head_dim = n_embed/nhead"
        "n_layer = 6 # number of transformer layers\n",
        "dropout = 0.2 # dropout rate\n",
        "torch.manual_seed(1337) # seed for reproducibility\n",
        "\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7TLuh5V2ok7"
      },
      "source": [
        "**Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ievOqfP2oVH"
      },
      "outputs": [],
      "source": [
        "# loading the tokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")# replace with your custom tokenizer if there or refer - https://colab.research.google.com/github/OE-Void/Tokenizer-from_scratch/blob/main/assets/Tokenizer_from_scratch.ipynb\n",
        "special_tokens_dict = { \"eos_token\": \"<eos>\", \"bos_token\": \"<bos>\", \"pad_token\": \"<pad>\" } # adding extra tokens\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "# eos token = end of text (teaches the model to stop the generation)\n",
        "# bos token = begin of text (teaches the model to start the generation)\n",
        "# pad token = padding token (pads the sequences to the maximum length) # attention mask will ignore the pad tokens\n",
        "# NOTE on Padding:\n",
        "# Although we added the <pad> token, it is UNUSED in the training data generation below.\n",
        "# We are using \"packed training\" (flattening all stories into one continuous stream),\n",
        "# so there is no need to pad sequences to equal lengths during training.\n",
        "# However, we define it here so it exists for inference/generation later or fine tuning for user assistant role.\n",
        "\n",
        "eos_id = tokenizer.eos_token_id\n",
        "bos_id = tokenizer.bos_token_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRItSiZxjcRg"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(tokenizer) # sets the vocab size of model to the size of tokenizer\n",
        "print(f\"Vocab size: {vocab_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBdfXhOEjcRg"
      },
      "source": [
        "- Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zo1srzpvjcRg"
      },
      "outputs": [],
      "source": [
        "# lets preprocess data for training\n",
        "\n",
        "dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train[:10000]\")\n",
        "\n",
        "print(\"Processing data...\")\n",
        "\n",
        "def encode_function(examples):\n",
        "    ids_list = []\n",
        "    for text in examples[\"text\"]: # change the text to desired row if dataset is changed by you\n",
        "        # We disable verbose to stop the length warning\n",
        "        # We allow it to go over max length because we will flatten it anyway\n",
        "        ids = tokenizer.encode(text, verbose=False)\n",
        "        ids = [bos_id] + ids + [eos_id] # manually adding eos and bos as tokenizer\n",
        "        ids_list.append(ids)\n",
        "    return {\"input_ids\": ids_list}\n",
        "\n",
        "tokenized_dataset = dataset.map(encode_function, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# Flatten (packing the sequence)\n",
        "\"\"\"example by Parvesh\n",
        "input_ids = [story1:[1, 2, 3, 4, 5], story2:[6, 7, 8, 9, 10], story3:[11, 12, 13, 14, 15]]\n",
        "\n",
        "flattened = list(itertools.chain(*input_ids))\n",
        "ids = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] all are added to a single list so model can learn from all the stories at once\n",
        "mainly learns the next token prediction as context is not carries in each story it depends on max seq length the context may be uneven\n",
        "eg. if max seq length = 1024, story = 800 tokens now model will complete the first story but as data is packed it will also need to predict the next half\n",
        "yup there will be disturbance of eos and bos tokens\n",
        "\"\"\"\n",
        "\n",
        "'''\n",
        "Example: By gemini\n",
        "  input_ids = [ [1, 2, 3], [4, 5, 6], [7, 8, 9] ]\n",
        "  # distinct stories: [story 1], [story 2], [story 3]\n",
        "\n",
        "  flattened = list(itertools.chain(*input_ids))\n",
        "  # ids = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "  # All stories are merged into one long stream of data.\n",
        "\n",
        "Why do we do this?\n",
        "1. Continuous Learning: The model learns from all stories at once without gaps.\n",
        "   It mainly focuses on \"next token prediction.\"\n",
        "\n",
        "2. The \"Uneven\" Context:\n",
        "   Since we mashed everything together, a single training block (batch) might look like this:\n",
        "\n",
        "   If block_size = 1024:\n",
        "   [ ... end of Story A (800 tokens) ... <bos> ... start of Story B (224 tokens) ... ]\n",
        "\n",
        "   The model learns that <bos> means \"reset\" â€” new context starts here!\n",
        "'''\n",
        "ids = list(itertools.chain(*tokenized_dataset[\"input_ids\"]))\n",
        "data = torch.tensor(ids, dtype=torch.long)\n",
        "\n",
        "# Split\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# this function is used to get a batchs of data\n",
        "# it returns x(input seq) and y(target seq) where y is x shifted by 1\n",
        "# eg. x = [1, 2, 3, 4, 5], y = [2, 3, 4, 5, 6] extra 6 in y means it have to predict that 6th token\n",
        "\n",
        "def get_batch(split):\n",
        "    data_source = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(0, len(data_source) - block_size - 1, (batch_size,))\n",
        "    x = torch.stack([data_source[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data_source[i+1:i+block_size+1] for i in ix])\n",
        "    return x.to(device), y.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcKowW3HjcRg"
      },
      "source": [
        "## Main model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pc0_8puY9SRy",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Model\n",
        "\n",
        "# this takes too much time to add comments so gemini added most of comments not entirely\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not needed but good practice\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities (basically sclaes values btw 0 and 1)\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT55BrnyjcRh"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLRDK2dTjcRh"
      },
      "source": [
        "- **initilizing the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEDPoIrZjcRh"
      },
      "outputs": [],
      "source": [
        "model = GPTLanguageModel() # we made the model using the model we defined earlier\n",
        "m = model.to(device) # we moved the model to the device cpu/cuda\n",
        "print(f\"{sum(p.numel() for p in m.parameters())/1e6:.2f} M parameters\") # counts the number of parameters in the model\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate) # lr is the learning rate we defined in the hyperpaprameter section optimizer for the model (AdamW is used for training the model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Trainer**"
      ],
      "metadata": {
        "id": "nbKYhyAJkHSL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7HHD481jcRh"
      },
      "outputs": [],
      "source": [
        "# a not soo basic training loop\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}# empty dictionary to store the losses\n",
        "    model.eval()# we set the model to evaluation mode\n",
        "    for split in [\"train\", \"val\"]:# we iterate over the train and val splits\n",
        "        losses = torch.zeros(eval_iters)# we create a tensor of zeros to store the losses\n",
        "        for k in range(eval_iters):# we iterate over the eval_iters\n",
        "            X, Y = get_batch(split)# we get a batch of data\n",
        "            logits, loss = model(X, Y)# we get the logits and loss\n",
        "            losses[k] = loss.item()# we store the loss\n",
        "        out[split] = losses.mean()\n",
        "    model.train()# we set the model to train mode\n",
        "    return out\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for iter in range(max_iters):# we iterate over the max_iters that we set on hyperparameters\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:# we check if the iteration is a multiple of eval_interval or if it is the last iteration\n",
        "        losses = estimate_loss()# we estimate the loss\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")# we print the loss\n",
        "    xb, yb = get_batch(\"train\")# we get a batch of data\n",
        "    logits, loss = model(xb, yb)# we get the logits and loss\n",
        "    optimizer.zero_grad(set_to_none=True)# we zero the gradients (this resets the old gradients)\n",
        "    loss.backward()# we compute the gradients (this is where the magic happens)\n",
        "    optimizer.step()# we update the weights based on the gradients\n",
        "\n",
        "print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPM4pWYEzlmF"
      },
      "source": [
        "## Infernce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkA0qvcVzkwp"
      },
      "outputs": [],
      "source": [
        "print(\"Generating text...\")\n",
        "# We start with the BOS token\n",
        "context = torch.tensor([[bos_id]], dtype=torch.long, device=device)\n",
        "generated_ids = m.generate(context, max_new_tokens=256)[0].tolist()\n",
        "print(tokenizer.decode(generated_ids))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8BHQSG9jcRi"
      },
      "source": [
        "## Saving model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16orhEzYjcRi"
      },
      "source": [
        "- imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhlJAuysjcRi"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from safetensors.torch import save_file\n",
        "from transformers import PreTrainedModel, PretrainedConfig, AutoTokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6hOrAcljcRi"
      },
      "source": [
        "- setting the essentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFaHFUJdjcRi"
      },
      "outputs": [],
      "source": [
        "\n",
        "output_dir = \"my_model\" # output directory\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True) # if you don't know what it does don't ask\n",
        "\n",
        "current_vocab_size = len(tokenizer)  # to avoid the vocab size mismatches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QL_kJBpKjcRi"
      },
      "outputs": [],
      "source": [
        "# @title Modelling file\n",
        "\n",
        "modeling_code = \"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from transformers import PreTrainedModel, PretrainedConfig\n",
        "\n",
        "class GPTCustomConfig(PretrainedConfig):\n",
        "    model_type = \"gpt_custom\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size=50257,\n",
        "        n_embd=384,\n",
        "        block_size=4096,\n",
        "        n_head=6,\n",
        "        n_layer=6,\n",
        "        dropout=0.2,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.n_embd = n_embd\n",
        "        self.block_size = block_size\n",
        "        self.n_head = n_head\n",
        "        self.n_layer = n_layer\n",
        "        self.dropout = dropout\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, config, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(config.n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(config.n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(config.n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(config.block_size, config.block_size)))\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        head_size = config.n_embd // config.n_head\n",
        "        self.heads = nn.ModuleList([Head(config, head_size) for _ in range(config.n_head)])\n",
        "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            nn.Dropout(config.dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.sa = MultiHeadAttention(config)\n",
        "        self.ffwd = FeedFoward(config)\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(PreTrainedModel):\n",
        "    config_class = GPTCustomConfig\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(config.block_size, config.n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.post_init()\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, input_ids, labels=None, attention_mask=None, **kwargs):\n",
        "        # Renaming inputs to match Hugging Face standards (input_ids, labels)\n",
        "        B, T = input_ids.shape\n",
        "        device = input_ids.device\n",
        "\n",
        "        tok_emb = self.token_embedding_table(input_ids)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            B, T, C = logits.shape\n",
        "            # Flatten for CrossEntropy\n",
        "            shift_logits = logits.view(B*T, C)\n",
        "            shift_labels = labels.view(B*T)\n",
        "            loss = F.cross_entropy(shift_logits, shift_labels)\n",
        "\n",
        "        return (loss, logits) if loss is not None else (logits,)\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ll1e0YPPjcRi"
      },
      "outputs": [],
      "source": [
        "# Write the python modeling file\n",
        "with open(os.path.join(output_dir, \"modeling_gpt_custom.py\"), \"w\") as f:\n",
        "    f.write(modeling_code)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAnArfxkjcRi"
      },
      "source": [
        "- **lets make the config.json of model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPGH5LcXjcRi"
      },
      "outputs": [],
      "source": [
        "config_dict = {\n",
        "    \"architectures\": [\"GPTLanguageModel\"],\n",
        "    \"model_type\": \"gpt_custom\",\n",
        "    \"auto_map\": {\n",
        "        \"AutoConfig\": \"modeling_gpt_custom.GPTCustomConfig\",\n",
        "        \"AutoModel\": \"modeling_gpt_custom.GPTLanguageModel\",\n",
        "        \"AutoModelForCausalLM\": \"modeling_gpt_custom.GPTLanguageModel\"\n",
        "    },\n",
        "    \"vocab_size\": current_vocab_size,\n",
        "    \"n_embd\": n_embd,           # From hyperparemters\n",
        "    \"block_size\": block_size,   # From hyperparemters\n",
        "    \"n_head\": n_head,           # From hyperparemters\n",
        "    \"n_layer\": n_layer,         # From hyperparemters\n",
        "    \"dropout\": dropout,         # From hyperparemters\n",
        "    \"bos_token_id\": bos_id,\n",
        "    \"eos_token_id\": eos_id,\n",
        "    \"pad_token_id\": tokenizer.pad_token_id\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhNer1lSjcRi"
      },
      "outputs": [],
      "source": [
        "# @title save config.json\n",
        "print(\"Saving config.json...\")\n",
        "with open(os.path.join(output_dir, \"config.json\"), \"w\") as f:\n",
        "    json.dump(config_dict, f, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ST0YoYOpjcRi"
      },
      "outputs": [],
      "source": [
        "print(\"Saving model weights...\")\n",
        "# We use safetensors for efficiency\n",
        "save_file(model.state_dict(), os.path.join(output_dir, \"model.safetensors\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7H-vsy4jcRi"
      },
      "outputs": [],
      "source": [
        "print(\"Saving tokenizer...\")\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(f\"Model saved successfully to '{output_dir}/'\")\n",
        "print(\"=\"*50)\n",
        "print(\"HOW TO LOAD THIS MODEL:\")\n",
        "print(\"from transformers import AutoModelForCausalLM, AutoTokenizer\")\n",
        "print(f\"model = AutoModelForCausalLM.from_pretrained('{output_dir}', trust_remote_code=True)\")\n",
        "print(f\"tokenizer = AutoTokenizer.from_pretrained('{output_dir}')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Save the Model to hf**"
      ],
      "metadata": {
        "id": "_E7s67G5jwLb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he5h4diwjcRi"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import upload_folder\n",
        "# update it for your case\n",
        "upload_folder(\n",
        "    folder_path=\"./model\",\n",
        "    repo_id=\"oe-void/transformers\",\n",
        "    repo_type=\"model\",\n",
        "    token=\"hf_\",\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
