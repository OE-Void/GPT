# GPT From Scratch

[`https://colab.research.google.com/github/OE-Void/GPT/blob/main/model_from_scr.ipynb`](https://colab.research.google.com/github/OE-Void/GPT/blob/main/model_from_scr.ipynb)

A **PyTorch implementation of a GPT-style language model**, built from scratch for **educational purposes** and **scalable usage**.  
This project demonstrates how transformer-based language models can be trained, evaluated, and deployed.

---

## âœ¨ Features
- Minimal, modular PyTorch implementation of GPT
- Configurable hyperparameters (`n_embd`, `n_layer`, `n_head`, etc.)
- Training loop with evaluation and checkpoint saving
- Hugging Face integration for easy upload and inference
- Colab notebook for quick experimentation

---

## ğŸš€ Usage After Training (with Hugging Face)

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load model and tokenizer from Hugging Face Hub
model = AutoModelForCausalLM.from_pretrained('your_repo_id', trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained('your_repo_id')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

print("Generating text...")

# Start with BOS token
context = torch.tensor([[tokenizer.bos_token_id]], dtype=torch.long, device=device)

# Generate sequence
generated_ids = model.generate(context, max_new_tokens=256)[0].tolist()
print(tokenizer.decode(generated_ids))
```

---

## ğŸ“‚ Project Structure

```
GPT/
â”œâ”€â”€ configs/     # Configuration files (config.py)
â”œâ”€â”€ data/        # Data loading and preprocessing (dataset.py)
â”œâ”€â”€ model/       # Model definition (model.py)
â”œâ”€â”€ trainer/     # Training loop and saving logic (trainer.py)
â””â”€â”€ main.py      # Entry point for training and generation
```

---

## âš™ï¸ Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/OE-Void/GPT.git
   cd GPT
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

---

## ğŸ‹ï¸ Training

To train the model:

```bash
python -m GPT.main
```

The trained model will be saved in the `my_model` directory.

---

## ğŸ”§ Configuration

Edit `GPT/configs/config.py` to adjust hyperparameters such as:
- `n_embd` â†’ embedding dimension size
- `n_layer` â†’ number of transformer layers
- `n_head` â†’ number of attention heads
- `block_size` â†’ maximum sequence length
- `batch_size` â†’ training batch size

---

## ğŸ¤ Contributing
Pull requests are welcome! For major changes, please open an issue first to discuss what youâ€™d like to change.

---

## ğŸ“œ License
This project is licensed under the MIT License â€” see the `LISENCE` file for details.

